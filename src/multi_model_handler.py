"""Multi-model handler for vision + conversation with smart detection (Async)."""

from typing import Optional
from .vision_handler import VisionHandler
from . import config
import aiohttp
import asyncio
import time
from .logger import logger

class MultiModelHandler:
    """Handle conversations using both vision (Moondream) and text (Llama) models (Async)."""
    
    def __init__(self):
        self.vision = VisionHandler()
        self.text_model = config.OLLAMA_MODEL
        self.ollama_url = config.OLLAMA_BASE_URL
        self.conversation_history = []
        self.vision_context = None
        self.last_vision_time = 0
        
        logger.info("тЬЕ Multi-model system ready (Moondream + Llama)")

    async def initialize(self):
        """Initialize vision system."""
        await self.vision.initialize()
    
    def _needs_vision(self, command: str) -> bool:
        """Intelligently detect if query needs vision/camera."""
        cmd = command.lower()
        
        # Explicit vision triggers
        explicit_triggers = [
            'what do you see', 'what can you see', 'look at', 'describe what you see',
            'tell me what you see', 'camera', 'take a look', 'show me',
            'рдХреНрдпрд╛ рджрд┐рдЦ рд░рд╣рд╛ рд╣реИ', 'рдХреНрдпрд╛ рджреЗрдЦ рд░рд╣рд╛ рд╣реИ', 'рдХреНрдпрд╛ рджреАрдЦ рд░рд╣рд╛ рд╣реИ',
            'рджреЗрдЦреЛ', 'рджрд┐рдЦрд╛рдУ', 'рджреЗрдЦ', 'рдХреНрдпрд╛ рд╣реИ',
            'рдпрд╣ рдХреНрдпрд╛ рд╣реИ', 'рдпреЗ рдХреНрдпрд╛ рд╣реИ',
            'рдХреИрдорд░рд╛', 'рдХреИрдорд░реЗ рд╕реЗ рджреЗрдЦреЛ',
            'ркдркорлЗ рк╢рлБркВ ркЬрлБркУ ркЫрлЛ', 'рк╢рлБркВ ркжрлЗркЦрк╛ркп ркЫрлЗ', 'рк╢рлБркВ ркЬрлЛрк╡рк╛ ркорк│рлЗ ркЫрлЗ',
            'ркЬрлБркУ', 'ркжрлЗркЦрк╛рк╡рлЛ', 'рк╢рлБркВ ркЫрлЗ',
            'ркЖ рк╢рлБркВ ркЫрлЗ', 'ркП рк╢рлБркВ ркЫрлЗ',
            'ркХрлЗркорлЗрк░рк╛', 'ркХрлЗркорлЗрк░рк╛ркерлА ркЬрлБркУ'
        ]
        
        for trigger in explicit_triggers:
            if trigger in cmd:
                logger.info(f"тЬЕ Vision trigger matched: '{trigger}'")
                return True
        
        # Questions about visible things
        vision_question_patterns = [
            'how many people', 'how many person', 'who is', 'who are',
            'is there a person', 'are there people', 'anyone here',
            'рдХрд┐рддрдиреЗ рд▓реЛрдЧ', 'рдХрд┐рддрдиреЗ рд╡реНрдпрдХреНрддрд┐', 'рдХреМрди рд╣реИ', 'рдХреЛрдИ рд╡реНрдпрдХреНрддрд┐',
            'ркХрлЗркЯрк▓рк╛ рк▓рлЛркХрлЛ', 'ркХрлЛркг ркЫрлЗ', 'ркХрлЛркИ рк╡рлНркпркХрлНркдрк┐',
            'what object', 'what is on', 'what is in front',
            'how many object', 'count the', 'objects in',
            'рдХреНрдпрд╛ рд╡рд╕реНрддреБ', 'рдХрд┐рддрдиреА рд╡рд╕реНрддреБ', 'рдХрд┐рддрдиреЗ рдЪреАрдЬ',
            'рк╢рлБркВ рк╡рк╕рлНркдрлБ', 'ркХрлЗркЯрк▓рлА рк╡рк╕рлНркдрлБ',
            'what color', 'what colour', 'color of',
            'рдХреНрдпрд╛ рд░рдВрдЧ', 'рд░рдВрдЧ рдХреИрд╕рд╛', 'ркХркпрлЛ рк░ркВркЧ', 'рк░ркВркЧ рк╢рлБркВ',
            'where is', 'where am i', 'what room', 'what place',
            'рдХрд╣рд╛рдБ рд╣реИ', 'рдХрд╣рд╛рдБ рд╣реВрдБ', 'рдХреМрди рд╕рд╛ рдХрдорд░рд╛', 'ркХрлНркпрк╛ркВ ркЫрлЗ', 'ркХрлНркпрк╛ркВ ркЫрлБркВ',
            'describe the', 'describe this', 'what is this',
            'tell me about this', 'read this', 'what does it say',
            'рдмрддрд╛рдУ рдпрд╣', 'рдпрд╣ рдХреНрдпрд╛', 'рк╡рк░рлНркгрки ркХрк░рлЛ', 'ркЖ рк╢рлБркВ',
            'read the text', 'what does it say', 'read what',
            'рдкрдврд╝реЛ', 'рдХреНрдпрд╛ рд▓рд┐рдЦрд╛ рд╣реИ', 'рк╡рк╛ркВркЪрлЛ', 'рк╢рлБркВ рк▓ркЦрлНркпрлБркВ ркЫрлЗ'
        ]
        
        for pattern in vision_question_patterns:
            if pattern in cmd:
                logger.info(f"тЬЕ Vision pattern matched: '{pattern}'")
                return True
        
        # Check for visual context questions
        visual_nouns = [
            'person', 'people', 'face', 'man', 'woman',
            'object', 'thing', 'item',
            'room', 'wall', 'door', 'window',
            'рд╡реНрдпрдХреНрддрд┐', 'рд▓реЛрдЧ', 'рд╡рд╕реНрддреБ', 'рдХрдорд░рд╛', 'рджреАрд╡рд╛рд░',
            'рк▓рлЛркХрлЛ', 'рк╡рк╕рлНркдрлБ', 'рк░рлВрко', 'ркжрк┐рк╡рк╛рк▓'
        ]
        
        question_words = [
            'what', 'how many', 'which', 'where', 'is there',
            'рдХреНрдпрд╛', 'рдХрд┐рддрдиреЗ', 'рдХрд╣рд╛рдБ', 'ркХркпрлБркВ', 'ркХрлЗркЯрк▓рк╛', 'ркХрлНркпрк╛ркВ'
        ]
        
        has_question = any(q in cmd for q in question_words)
        has_visual_noun = any(noun in cmd for noun in visual_nouns)
        
        if has_question and has_visual_noun:
            logger.info(f"тЬЕ Visual question detected: question + visual noun")
            return True
        
        # Fallback: if contains vision-related verbs
        vision_verbs = [
            'see', 'look', 'watch', 'view', 'show',
            'рджреЗрдЦ', 'рджрд┐рдЦ', 'рджреАрдЦ', 'рджрд┐рдЦрд╛', 'рджреЗрдЦреЛ',
            'ркЬрлБркУ', 'ркЬрлЛрк╡рлБркВ', 'ркжрлЗркЦрк╛рк╡рлЛ'
        ]
        
        if any(verb in cmd for verb in vision_verbs):
            if '?' in cmd or any(w in cmd for w in ['what', 'рдХреНрдпрд╛', 'ркХркпрлБркВ', 'ркХрлЗрко']):
                logger.info(f"тЬЕ Vision verb + question detected")
                return True
        
        return False

    async def process_vision_command(self, command: str, language: str = 'en') -> Optional[str]:
        """Process commands that need vision with smart question generation (Async)."""
        
        if not self._needs_vision(command):
            return None
        
        logger.info("ЁЯУ╕ Vision required - activating camera")
        
        question = self._generate_vision_question(command)
        logger.info(f"ЁЯСБя╕П  Asking Moondream: '{question}'")
        
        # Await the async vision call
        description = await self.vision.see_and_describe(question)
        
        if description:
            self.vision_context = description
            self.last_vision_time = time.time()
            
            if language != 'en':
                translated_description = await self._translate_vision_response(description, language)
            else:
                translated_description = description
            
            if language == 'hi':
                response = f"рдореИрдВ рджреЗрдЦ рд░рд╣рд╛ рд╣реВрдВ: {translated_description}"
            elif language == 'gu':
                response = f"рк╣рлБркВ ркЬрлЛркИ рк░рк╣рлНркпрлЛ ркЫрлБркВ: {translated_description}"
            else:
                response = f"I can see: {translated_description}"
            
            return response
        else:
            if language == 'hi':
                return "рдореИрдВ рдХреИрдорд░рд╛ рдПрдХреНрд╕реЗрд╕ рдирд╣реАрдВ рдХрд░ рдкрд╛ рд░рд╣рд╛ рд╣реВрдВ"
            elif language == 'gu':
                return "рк╣рлБркВ ркХрлЗркорлЗрк░рк╛ркирлЗ ркНркХрлНрк╕рлЗрк╕ ркХрк░рлА рк╢ркХркдрлЛ ркиркерлА"
            else:
                return "I cannot access the camera right now"

    async def _translate_vision_response(self, english_text: str, target_language: str) -> str:
        """Translate vision response from English to target language using Llama (Async)."""
        try:
            if target_language == 'hi':
                prompt = f"""Translate the following English text to Hindi. Only provide the Hindi translation, nothing else.

    English: {english_text}

    Hindi:"""
            elif target_language == 'gu':
                prompt = f"""Translate the following English text to Gujarati. Only provide the Gujarati translation, nothing else.

    English: {english_text}

    Gujarati:"""
            else:
                return english_text
            
            payload = {
                "model": self.text_model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.3,
                    "num_predict": 300
                }
            }
            
            logger.info(f"ЁЯМР Translating to {target_language}...")
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.ollama_url}/api/generate",
                    json=payload,
                    timeout=30
                ) as response:
                    if response.status != 200:
                        logger.error(f"тЭМ Translation failed: {response.status}")
                        return english_text
                        
                    result = await response.json()
                    translation = result.get("response", "").strip()
            
            if translation:
                logger.info(f"тЬЕ Translated: {translation[:100]}...")
                return translation
            else:
                return english_text
                
        except Exception as e:
            logger.warning(f"тЪая╕П Translation failed: {e}, using English")
            return english_text
    
    def _generate_vision_question(self, command: str) -> str:
        """Generate appropriate question for Moondream based on user query."""
        cmd = command.lower()
        
        if any(word in cmd for word in ['time', 'clock', 'watch', 'рд╕рдордп', 'рдШрдбрд╝реА', 'рк╕ркоркп', 'ркШркбрк┐ркпрк╛рк│']):
            return "What time is shown on any clock, watch, or time display visible in this image? Read the exact time."
        
        if 'how many people' in cmd or 'how many person' in cmd or 'рдХрд┐рддрдиреЗ рд▓реЛрдЧ' in cmd or 'ркХрлЗркЯрк▓рк╛ рк▓рлЛркХрлЛ' in cmd:
            return "How many people are in this image? Count them carefully."
        
        if 'who is' in cmd or 'who are' in cmd or 'рдХреМрди рд╣реИ' in cmd or 'ркХрлЛркг ркЫрлЗ' in cmd:
            return "Describe the people in this image. Who do you see?"
        
        if 'how many object' in cmd or 'count' in cmd or 'рдХрд┐рддрдиреЗ' in cmd or 'ркХрлЗркЯрк▓рк╛' in cmd:
            return "Count and list all the objects you can see in this image."
        
        if 'what color' in cmd or 'what colour' in cmd or 'рд░рдВрдЧ' in cmd or 'рк░ркВркЧ' in cmd:
            return "What colors do you see in this image? Describe them in detail."
        
        if 'where am i' in cmd or 'what room' in cmd or 'what place' in cmd or 'рдХрд╣рд╛рдБ' in cmd or 'ркХрлНркпрк╛ркВ' in cmd:
            return "Describe this location. What kind of room or place is this? What can you see?"
        
        if any(word in cmd for word in ['read', 'text', 'written', 'рдкрдврд╝', 'рд▓рд┐рдЦрд╛', 'рк╡рк╛ркВркЪ', 'рк▓ркЦрлНркпрлБркВ']):
            return "Read all the text visible in this image. What does it say?"
        
        if 'what is this' in cmd or 'what is on' in cmd or 'what is in' in cmd:
            return "Describe the main object or item in this image in detail."
        
        if 'weather' in cmd or 'temperature' in cmd or 'рдореМрд╕рдо' in cmd or 'рк╣рк╡рк╛ркорк╛рки' in cmd:
            return "If there's a weather display or temperature reading visible, what does it show?"
        
        if 'price' in cmd or 'cost' in cmd or 'how much' in cmd or 'рдХреАрдордд' in cmd or 'ркХрк┐ркВркоркд' in cmd:
            return "If there are any prices or monetary values visible, what are they?"
        
        if any(word in cmd for word in ['screen', 'monitor', 'display', 'phone', 'рд╕реНрдХреНрд░реАрди', 'рк╕рлНркХрлНрк░рлАрки']):
            return "What is displayed on the screen or monitor in this image? Describe what you see."
        
        if 'sign' in cmd or 'label' in cmd or 'рд╕рд╛рдЗрди' in cmd or 'рк▓рлЗркмрк▓' in cmd:
            return "What signs, labels, or text are visible in this image? Read them."
        
        if any(phrase in cmd for phrase in ['what do you see', 'what can you see', 'describe', 
                                            'рджрд┐рдЦ рд░рд╣рд╛', 'рджреЗрдЦ рд░рд╣рд╛', 'ркЬрлБркУ ркЫрлЛ']):
            return "Describe everything visible in this image in detail - including people, objects, text, numbers, colors, and any information displayed."
        
        question = command.strip()
        question_words = ['what', 'how', 'where', 'when', 'why', 'who', 'which',
                        'рдХреНрдпрд╛', 'рдХреИрд╕реЗ', 'рдХрд╣рд╛рдБ', 'рдХрдм', 'рдХреНрдпреЛрдВ', 'рдХреМрди',
                        'рк╢рлБркВ', 'ркХрлЗрко', 'ркХрлНркпрк╛ркВ', 'ркХрлНркпрк╛рк░рлЗ', 'ркХрлЗрко', 'ркХрлЛркг']
        
        if any(question.lower().startswith(qw) for qw in question_words):
            return f"Answer this question about the image: {question}"
        
        return "Describe everything you see in this image in detail, including any text, numbers, people, objects, colors, and the setting. Be specific and thorough."

    async def generate_with_vision_context(self, user_input: str, language: str = 'en') -> Optional[str]:
        """Generate response using vision context if available (Async)."""
        try:
            prompt_parts = []
            
            if language == 'hi':
                prompt_parts.append("рддреБрдо рдПрдХ рд╕рд╣рд╛рдпрдХ AI рд╣реЛ рдЬреЛ рджреГрд╢реНрдп рдЬрд╛рдирдХрд╛рд░реА рдХреЛ рд╕рдордЭ рд╕рдХрддрд╛ рд╣реИред\n\n")
            elif language == 'gu':
                prompt_parts.append("ркдркорлЗ ркПркХ рк╕рк╣рк╛ркпркХ AI ркЫрлЛ ркЬрлЗ ркжрлНрк░рк╢рлНркп ркорк╛рк╣рк┐ркдрлА рк╕ркоркЬрлА рк╢ркХрлЗ ркЫрлЗ.\n\n")
            else:
                prompt_parts.append("You are a helpful AI assistant with access to visual information.\n\n")
            
            if self.vision_context:
                if time.time() - self.last_vision_time < 60:
                    if language == 'hi':
                        prompt_parts.append(f"рджреГрд╢реНрдп рдЬрд╛рдирдХрд╛рд░реА (рдХреИрдорд░рд╛): {self.vision_context}\n\n")
                    elif language == 'gu':
                        prompt_parts.append(f"ркжрлНрк░рк╢рлНркп ркорк╛рк╣рк┐ркдрлА (ркХрлЗркорлЗрк░рк╛): {self.vision_context}\n\n")
                    else:
                        prompt_parts.append(f"Visual information (from camera): {self.vision_context}\n\n")
            
            for msg in self.conversation_history[-4:]:
                if msg['role'] == 'user':
                    prompt_parts.append(f"User: {msg['content']}\n")
                else:
                    prompt_parts.append(f"Assistant: {msg['content']}\n")
            
            prompt_parts.append(f"User: {user_input}\n")
            prompt_parts.append("Assistant: ")
            
            prompt = "".join(prompt_parts)
            
            payload = {
                "model": self.text_model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "num_predict": 200
                }
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.ollama_url}/api/generate",
                    json=payload,
                    timeout=60
                ) as response:
                    if response.status != 200:
                        logger.error(f"тЭМ Response generation failed: {response.status}")
                        return None
                        
                    result = await response.json()
                    assistant_response = result.get("response", "").strip()
            
            if assistant_response:
                self.conversation_history.append({
                    'role': 'user',
                    'content': user_input,
                    'language': language
                })
                self.conversation_history.append({
                    'role': 'assistant',
                    'content': assistant_response,
                    'language': language
                })
                
                if len(self.conversation_history) > 10:
                    self.conversation_history = self.conversation_history[-10:]
                
                return assistant_response
            
            return None
            
        except Exception as e:
            logger.error(f"тЭМ Response generation failed: {e}")
            return None
    
    def clear_vision_context(self):
        """Clear vision context."""
        self.vision_context = None
        self.last_vision_time = 0
        logger.info("ЁЯФД Vision context cleared")
